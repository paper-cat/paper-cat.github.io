I"<h1 id="character-level-cnn-paper-review">Character level CNN Paper review</h1>
<hr />
<h2 id="기본-소개">기본 소개</h2>

<p><a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626 논문 링크</a></p>

<p>자연어 처리의 대부분의 Task 는 단어 기반으로 되어 있습니다. 2016년 당시에도 word2vec 같이 대부분의 연구가 word 기반으로 이루어졌습니다. 
이 논문에서는, 여러 convolutional network 기반 연구가 많음에 기반해 character 단위의 Text Classification 모델을 연구해 제안합니다.</p>

<h2 id="character-level-convolutional-network">Character-level convolutional Network</h2>

<h3 id="key-module">Key Module</h3>

<p>Convolution Layer 를 key module 로 사용하고 있다고 설명합니다. (자세한 내용은 수식으로 되어 있는데)</p>

<p><img src="../assets/post_img/paper/paper2_1.png" alt="conv수식" /></p>

<p>\(c\) : offset constant (padding)</p>

<p>\(d\) : depth</p>

<p>\(f_{ij}(x)\) : kernel function (weights) (i=1,2,….,m, j = 1,2,….n)</p>

<p>\(g_{i}\) : input features with m input feature size</p>

<p>\(h_{j}\) : output features with n output feature size</p>

<p>\(h_{j}(y)\) : output by sum over i between \(g_{i}(x), f_{ij}(x)\)</p>

<p>복잡하게 써놨지만, 일반적인 convolutional layer 와 같습니다.</p>

<p>또 다른 key module 로 max pooling 을 언급합니다. Max pooling 덕분에 6개 이상의 깊은 모델을 학습할 수 있다고 합니다.</p>

<p>이외에 Relu 를 사용했고, optimizer 로 SGD 를, batch size 128 등등을 사용했다고 합니다.</p>

<h3 id="character-quantization">Character Quantization</h3>

<p>Input 으로는, encode (one-hot encoding) 된 character 들을 일정 길이 만큼 받아서 학습한다고 합니다. 모르는 character나 빈칸의 경우는 0으로 encoding 합니다.</p>

<p>알파벳, 숫자, 몇가지의 특수문자들을 다 합쳐서 총 70개의 character 를 사용했다고 합니다. 대문자 소문자의 경우도 후에 실험해서 비교합니다.</p>

<h3 id="model-design">Model Design</h3>

<p><img src="../assets/post_img/paper/paper2_2.png" alt="model" /></p>

<p>70개의 character 를 사용했기 때문에 features 의 개수는 70개가 됩니다. 그리고 한 문장의 길이를 1014 로 고정했는데, 이정도면 대부분의 문장의 의미를 담을 수 있기 때문이라고 합니다.</p>

<p>마지막 3개의 Fully connected layer 사이에 2개의 Dropout 을 넣어 Regularize 했습니다.</p>

<p>각 Convolution layer 와 Fully Connected Layer의 Feature 수는 아래와 같습니다.</p>

<p><img src="../assets/post_img/paper/paper2_3.png" alt="cnn_units" /></p>

<p><img src="../assets/post_img/paper/paper2_4.png" alt="dense_units" /></p>

<h3 id="data-augmentation-using-thesaurus">Data Augmentation using Thesaurus</h3>

<p>Data Augmentation 은 대부분의 Deep Learning 에 중요한 부분이다. 이 연구에서는 Data Augmentation 을 위해서 Thesaurus 라는 동의어 사전을 사용해서 문장의 단어들을 동의어로 변경한 뒤에 학습을 시켰다고 합니다.</p>

<p>Text 분야에서 Data Augmentation 은 방법이 많지도 않고 효과적인 방법이 거의 없어서 개인적으로는 잘 사용하지 않는다… 근데 찾아보다 보니 Text Augmentation에 관한 방법에 대한 논문들이 있어서 나중에 공부해서 다루도록 하겠습니다.</p>

<h2 id="result">Result</h2>

<p><img src="../assets/post_img/paper/paper2_5.png" alt="result" /></p>

<p>모델, 데이터셋 별로 Error 비율을 나타낸 표입니다.</p>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>

:ET